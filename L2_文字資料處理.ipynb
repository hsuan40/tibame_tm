{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 英文斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'book']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'this is a book'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x104638408>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.cut('酸民婉君也可以報名嗎?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/46/b7dzk4mn6g54qzptv608w7d00000gn/T/jieba.cache\n",
      "Loading model cost 1.133 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['酸民婉君', '也', '可以', '報名', '嗎', '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut('酸民婉君也可以報名嗎?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator (補充資料)\n",
    "\n",
    "generator是一種特別的iterator，可以透過yield 讓我們自行控制iterator 的狀態，決定是否要呼叫__next__讓他進行迭代行為。Generator 採取Lazy Evaluation，真正需要資料時才進行計算。\n",
    "\n",
    "優點：\n",
    "\n",
    "1. 不需要定義__next，提高程式碼可讀性\n",
    "2. 減少記憶體的使用\n",
    "3. 可以用'yield from'實作遞迴的迭代行為\n",
    "4. 可以建立一個可拆式的資料管線，減少了不同工作階段的耦合性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 大/ 巨蛋/ 案/ 對/ 市府/ 同仁/ 下/ 封口/ 封口令/ 口令/ ？/ 　/ / 柯/ P/ 否/ 認\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 　/ 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidchiu/.pyenv/versions/3.7.3/lib/python3.7/site-packages/jieba/dict.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.get_dict_file().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"userdict.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba 繁體\n",
    "-  https://github.com/ldkrsi/jieba-zh_TW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 a\n",
      "巨蛋 n\n",
      "案 ng\n",
      "對 p\n",
      "市府 n\n",
      "同仁 nr\n",
      "下 f\n",
      "封口令 n\n",
      "？ x\n",
      "　 x\n",
      "柯 nr\n",
      "P eng\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"大巨蛋案對市府同仁下封口令？　柯P否認\")\n",
    "for w in words:\n",
    "    print(w.word, w.flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 　/ 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "sentence = \"大巨蛋案對市府同仁下封口令？　柯P否認\"\n",
    "words = jieba.cut(sentence, cut_all=False)\n",
    "print(\"/ \".join(words))\n",
    "\n",
    "jieba.add_word('柯P',100, 'nr')\n",
    "jieba.add_word('大巨蛋',100, 'ns')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ckiptagger\n",
    "- https://github.com/ckiplab/ckiptagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "ws = WS(\"./data\")\n",
    "pos = POS(\"./data\")\n",
    "ner = NER(\"./data\")\n",
    "sentence_list = [\n",
    "    \"全聯福利中心…\",\n",
    "]\n",
    "\n",
    "word_sentence_list = ws(\n",
    "    sentence_list,\n",
    ")\n",
    "pos_sentence_list = pos(word_sentence_list)\n",
    "entity_sentence_list = ner(word_sentence_list, pos_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_weight = {\n",
    "    \"土地公\": 1,\n",
    "    \"土地婆\": 1,\n",
    "    \"公有\": 2,\n",
    "    \"\": 1,\n",
    "    \"來亂的\": \"啦\",\n",
    "    \"緯來體育台\": 1,\n",
    "}\n",
    "dictionary = construct_dictionary(word_to_weight)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4ced769a2e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"On the day when the government's top infectious disease specialist Anthony Fauci said he would not be surprised to see the US record 100,000 new coronavirus cases per day, Trump refused to break his deafening silence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = \"\"\"On the day when the government's top infectious disease specialist Anthony Fauci said he would not be surprised to see the US record 100,000 new coronavirus cases per day, Trump refused to break his deafening silence.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = [word_tokenize(sent) for sent in sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')\n",
    "s.stem(\"imaging\")\n",
    "s.stem(\"image\")\n",
    "s.stem(\"imagination\")\n",
    "s.stem(\"imagine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字雲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
